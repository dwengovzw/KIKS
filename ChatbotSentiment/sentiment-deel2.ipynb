{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../.images/ChatbotSentiment/bannerUGentDwengo.png\" alt=\"BannerUGentDwengo\" style=\"width:250px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='color: #690027;' markdown=\"1\">\n",
    "    <h1>SENTIMENTANALYSE</h1>\n",
    "    <h1>deel 2</h1> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-success\">\n",
    "In deze notebook zal je bij gegeven teksten (de data) onderzoek doen naar sentimentwoorden m.b.v. kunstmatige intelligentie (KI of AI). Je zal immers een <em>machine learning</em> model gebruiken. Dit model werd getraind met geannoteerde teksten en kan met grote nauwkeurigheid een tekst tokeniseren en van elk token de part-of-speech tag en het lemma bepalen. Je gebruikt een <em>regelgebaseerd AI-systeem</em> om het sentiment van de gegeven tekst te bepalen. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deel 1 maakte je kennis met de principes van een regelgebaseerde sentimentanalyse. \n",
    "\n",
    " -  Je maakt gebruik van een (bestaand) **lexicon** of woordenboek met daarin woorden gekoppeld aan hun **polariteit** (positief, negatief of neutraal).\n",
    " -  Voor je sentimentwoorden uit een lexicon kunt matchen met de data moet je de data inlezen en **preprocessen**.\n",
    " -  Veelvoorkomende preprocessing stappen zijn **lowercasing**, **tokenisering**,  **part-of-speech tagging** en  **lemmatisering**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-success\">\n",
    "In deze notebook zal je de uitvoer van de sentimentanalyse <b>automatiseren</b>. Je zal m.a.w. de computer het werk laten doen: de computer zal de data voorbereiden met een <em>machine learning model</em>, en met een <em>regelgebaseerd AI-systeem</em> de tokens matchen met het gegeven lexicon en een eindbeslissing nemen over het sentiment van de gegeven tekst. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules, model en lexicon inladen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor je aan de slag gaat, voorzie je eerst de nodige tools. \n",
    "\n",
    "-  Je importeert de nodige modules (dit hoef je maar één keer te doen). <br>Deze modules bevatten functies en methodes die jouw onderzoek zullen vergemakkelijken. Er zijn immers reeds zaken voorgeprogrammeerd, waardoor jij met vrij eenvoudige instructies kunt werken.\n",
    "-  Je laadt een machine learning model in om straks te gebruiken.\n",
    "-  Je leest ook al een sentimentlexicon in. \n",
    "\n",
    "Voer daartoe de drie code-cellen hieronder uit. De code in deze cellen hoef je niet te begrijpen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules importeren\n",
    "import pickle                     # voor lexicon\n",
    "from colorama import Fore, Back   # om in kleur te kunnen printen\n",
    "import spacy                      # voor getrainde modellen voor preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine learning model inladen\n",
    "nlp = spacy.load(\"nl_core_news_sm\")    # nlp staat voor natural language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-91f06da24cfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# lexicon inlezen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../.data/ChatbotSentiment/new_lexicondict.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#bestand 'lexicondict.pickle' in map 'lexicon' bevat het sentimentlexicon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlexicondict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "# lexicon inlezen \n",
    "with open('../.data/ChatbotSentiment/new_lexicondict.pickle', 'rb') as f: #bestand 'lexicondict.pickle' in map 'lexicon' bevat het sentimentlexicon\n",
    "    lexicondict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zo, je bent klaar voor stap 1: de data inlezen en bekijken. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='color: #690027;' markdown=\"1\">\n",
    "    <h2>1. De data inlezen</h2> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor deze opdracht zal je werken met dezelfde **klantenreview** als in deel 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stap 1: voer de volgende code-cel uit om de review in te lezen en vervolgens te bekijken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nieuw concept in Gent, maar dat kan volgens mij toch beter. De meeste cornflakes waren gewoon de basic soorten. Ook wat duur voor de hoeveelheid die je krijgt, vooral met de toppings zijn ze zuinig. En als je ontbijt aanbiedt, geef de mensen dan toch ook wat meer keuze voor hun koffie.\n"
     ]
    }
   ],
   "source": [
    "review = \"Nieuw concept in Gent, maar dat kan volgens mij toch beter. De meeste cornflakes waren gewoon de basic soorten. Ook wat duur voor de hoeveelheid die je krijgt, vooral met de toppings zijn ze zuinig. En als je ontbijt aanbiedt, geef de mensen dan toch ook wat meer keuze voor hun koffie.\"\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je bent klaar voor stap 2. \n",
    "\n",
    "In wat volgt laat je de computer de preprocessing op de review uitvoeren: lowercasing hadden we al geautomatiseerd in deel 1. Die code neem je over.  \n",
    "\n",
    "Je moet geen spaties toevoegen in de tekst, want het machine learning model zorgt voor het tokeniseren. Ook het part-of-speech taggen en lemmatiseren wordt nu geautomatiseerd m.b.v. het model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='color: #690027;' markdown=\"1\">\n",
    "    <h2>2. Preprocessing</h2> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zet tekst van de review om naar tekst in kleine letters met spaties voor en na de leestekens\n",
    "review_kleineletters = review.lower()  # review met kleine letters schrijven  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisatie, part-of-speech taggen en lemmatisering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De review **tokeniseren** en aan elk token een **part-of-speech** en een **lemma** toekennen, gebeurt automatisch met behulp van een daarvoor getraind model met een accuraatheid van 93 %! \n",
    "\n",
    "Je voert daarvoor de review (in kleine letters) in in het model `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_spatie in het model voeren\n",
    "doc = nlp(review_kleineletters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Van de review zijn nu de tokens bepaald en van elk token is het woord of leesteken zelf, de part-of-speech tag en het lemma opgeslagen in `doc`.  <br>\n",
    "Je bekijkt nu de tokens, de part-of-speech tags en de lemma's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elk token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token 'nieuw': nieuw\n",
      "token 'concept': concept\n",
      "token 'in': in\n",
      "token 'gent': gent\n",
      "token ',': ,\n",
      "token 'maar': maar\n",
      "token 'dat': dat\n",
      "token 'kan': kan\n",
      "token 'volgens': volgens\n",
      "token 'mij': mij\n",
      "token 'toch': toch\n",
      "token 'beter': beter\n",
      "token '.': .\n",
      "token 'de': de\n",
      "token 'meeste': meeste\n",
      "token 'cornflakes': cornflakes\n",
      "token 'waren': waren\n",
      "token 'gewoon': gewoon\n",
      "token 'de': de\n",
      "token 'basic': basic\n",
      "token 'soorten': soorten\n",
      "token '.': .\n",
      "token 'ook': ook\n",
      "token 'wat': wat\n",
      "token 'duur': duur\n",
      "token 'voor': voor\n",
      "token 'de': de\n",
      "token 'hoeveelheid': hoeveelheid\n",
      "token 'die': die\n",
      "token 'je': je\n",
      "token 'krijgt': krijgt\n",
      "token ',': ,\n",
      "token 'vooral': vooral\n",
      "token 'met': met\n",
      "token 'de': de\n",
      "token 'toppings': toppings\n",
      "token 'zijn': zijn\n",
      "token 'ze': ze\n",
      "token 'zuinig': zuinig\n",
      "token '.': .\n",
      "token 'en': en\n",
      "token 'als': als\n",
      "token 'je': je\n",
      "token 'ontbijt': ontbijt\n",
      "token 'aanbiedt': aanbiedt\n",
      "token ',': ,\n",
      "token 'geef': geef\n",
      "token 'de': de\n",
      "token 'mensen': mensen\n",
      "token 'dan': dan\n",
      "token 'toch': toch\n",
      "token 'ook': ook\n",
      "token 'wat': wat\n",
      "token 'meer': meer\n",
      "token 'keuze': keuze\n",
      "token 'voor': voor\n",
      "token 'hun': hun\n",
      "token 'koffie': koffie\n",
      "token '.': .\n"
     ]
    }
   ],
   "source": [
    "# token\n",
    "for token in doc:\n",
    "    print(f\"token '{token}': {token.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part of-speech tag 'nieuw': ADJ\n",
      "part of-speech tag 'concept': NOUN\n",
      "part of-speech tag 'in': ADP\n",
      "part of-speech tag 'gent': PROPN\n",
      "part of-speech tag ',': SYM\n",
      "part of-speech tag 'maar': CCONJ\n",
      "part of-speech tag 'dat': PRON\n",
      "part of-speech tag 'kan': VERB\n",
      "part of-speech tag 'volgens': ADP\n",
      "part of-speech tag 'mij': PRON\n",
      "part of-speech tag 'toch': ADV\n",
      "part of-speech tag 'beter': ADJ\n",
      "part of-speech tag '.': SYM\n",
      "part of-speech tag 'de': DET\n",
      "part of-speech tag 'meeste': ADV\n",
      "part of-speech tag 'cornflakes': NOUN\n",
      "part of-speech tag 'waren': AUX\n",
      "part of-speech tag 'gewoon': ADJ\n",
      "part of-speech tag 'de': DET\n",
      "part of-speech tag 'basic': ADJ\n",
      "part of-speech tag 'soorten': NOUN\n",
      "part of-speech tag '.': SYM\n",
      "part of-speech tag 'ook': ADV\n",
      "part of-speech tag 'wat': DET\n",
      "part of-speech tag 'duur': ADJ\n",
      "part of-speech tag 'voor': ADP\n",
      "part of-speech tag 'de': DET\n",
      "part of-speech tag 'hoeveelheid': NOUN\n",
      "part of-speech tag 'die': PRON\n",
      "part of-speech tag 'je': PRON\n",
      "part of-speech tag 'krijgt': AUX\n",
      "part of-speech tag ',': SYM\n",
      "part of-speech tag 'vooral': ADV\n",
      "part of-speech tag 'met': ADP\n",
      "part of-speech tag 'de': DET\n",
      "part of-speech tag 'toppings': NOUN\n",
      "part of-speech tag 'zijn': AUX\n",
      "part of-speech tag 'ze': PRON\n",
      "part of-speech tag 'zuinig': ADJ\n",
      "part of-speech tag '.': SYM\n",
      "part of-speech tag 'en': CCONJ\n",
      "part of-speech tag 'als': SCONJ\n",
      "part of-speech tag 'je': PRON\n",
      "part of-speech tag 'ontbijt': NOUN\n",
      "part of-speech tag 'aanbiedt': AUX\n",
      "part of-speech tag ',': SYM\n",
      "part of-speech tag 'geef': VERB\n",
      "part of-speech tag 'de': DET\n",
      "part of-speech tag 'mensen': NOUN\n",
      "part of-speech tag 'dan': ADV\n",
      "part of-speech tag 'toch': ADV\n",
      "part of-speech tag 'ook': ADV\n",
      "part of-speech tag 'wat': PRON\n",
      "part of-speech tag 'meer': DET\n",
      "part of-speech tag 'keuze': NOUN\n",
      "part of-speech tag 'voor': ADP\n",
      "part of-speech tag 'hun': PRON\n",
      "part of-speech tag 'koffie': NOUN\n",
      "part of-speech tag '.': SYM\n"
     ]
    }
   ],
   "source": [
    "# part-of-speech tag van elk token\n",
    "for token in doc:\n",
    "    print(f\"part of-speech tag '{token}': {token.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatisering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma 'nieuw': nieuw\n",
      "lemma 'concept': concept\n",
      "lemma 'in': in\n",
      "lemma 'gent': gent\n",
      "lemma ',': ,\n",
      "lemma 'maar': maar\n",
      "lemma 'dat': dat\n",
      "lemma 'kan': kunnen\n",
      "lemma 'volgens': volgens\n",
      "lemma 'mij': mij\n",
      "lemma 'toch': toch\n",
      "lemma 'beter': goed\n",
      "lemma '.': .\n",
      "lemma 'de': de\n",
      "lemma 'meeste': veel\n",
      "lemma 'cornflakes': cornflakes\n",
      "lemma 'waren': zijn\n",
      "lemma 'gewoon': gewoon\n",
      "lemma 'de': de\n",
      "lemma 'basic': basic\n",
      "lemma 'soorten': soort\n",
      "lemma '.': .\n",
      "lemma 'ook': ook\n",
      "lemma 'wat': wat\n",
      "lemma 'duur': duur\n",
      "lemma 'voor': voor\n",
      "lemma 'de': de\n",
      "lemma 'hoeveelheid': hoeveelheid\n",
      "lemma 'die': die\n",
      "lemma 'je': je\n",
      "lemma 'krijgt': krijgen\n",
      "lemma ',': ,\n",
      "lemma 'vooral': vooral\n",
      "lemma 'met': met\n",
      "lemma 'de': de\n",
      "lemma 'toppings': topping\n",
      "lemma 'zijn': zijn\n",
      "lemma 'ze': ze\n",
      "lemma 'zuinig': zuinig\n",
      "lemma '.': .\n",
      "lemma 'en': en\n",
      "lemma 'als': als\n",
      "lemma 'je': je\n",
      "lemma 'ontbijt': ontbijt\n",
      "lemma 'aanbiedt': aanbieden\n",
      "lemma ',': ,\n",
      "lemma 'geef': geven\n",
      "lemma 'de': de\n",
      "lemma 'mensen': mens\n",
      "lemma 'dan': dan\n",
      "lemma 'toch': toch\n",
      "lemma 'ook': ook\n",
      "lemma 'wat': wat\n",
      "lemma 'meer': meer\n",
      "lemma 'keuze': keuze\n",
      "lemma 'voor': voor\n",
      "lemma 'hun': hun\n",
      "lemma 'koffie': koffie\n",
      "lemma '.': .\n"
     ]
    }
   ],
   "source": [
    "# lemma van elk token\n",
    "for token in doc:\n",
    "    print(f\"lemma '{token}': {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maak lijsten van de tokens, lemma's en de part-of-speech tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deel 1 waren de lijsten van de lemma's en part-of-speech tags manueel opgemaakt. Nu kan dit automatisch omdat alle nodige info verzameld is in `doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens:\n",
      "['nieuw', 'concept', 'in', 'gent', ',', 'maar', 'dat', 'kan', 'volgens', 'mij', 'toch', 'beter', '.', 'de', 'meeste', 'cornflakes', 'waren', 'gewoon', 'de', 'basic', 'soorten', '.', 'ook', 'wat', 'duur', 'voor', 'de', 'hoeveelheid', 'die', 'je', 'krijgt', ',', 'vooral', 'met', 'de', 'toppings', 'zijn', 'ze', 'zuinig', '.', 'en', 'als', 'je', 'ontbijt', 'aanbiedt', ',', 'geef', 'de', 'mensen', 'dan', 'toch', 'ook', 'wat', 'meer', 'keuze', 'voor', 'hun', 'koffie', '.']\n",
      "lemma's:\n",
      "['nieuw', 'concept', 'in', 'gent', ',', 'maar', 'dat', 'kunnen', 'volgens', 'mij', 'toch', 'goed', '.', 'de', 'veel', 'cornflakes', 'zijn', 'gewoon', 'de', 'basic', 'soort', '.', 'ook', 'wat', 'duur', 'voor', 'de', 'hoeveelheid', 'die', 'je', 'krijgen', ',', 'vooral', 'met', 'de', 'topping', 'zijn', 'ze', 'zuinig', '.', 'en', 'als', 'je', 'ontbijt', 'aanbieden', ',', 'geven', 'de', 'mens', 'dan', 'toch', 'ook', 'wat', 'meer', 'keuze', 'voor', 'hun', 'koffie', '.']\n",
      "part-of-speech tags:\n",
      "['ADJ', 'NOUN', 'ADP', 'PROPN', 'SYM', 'CCONJ', 'PRON', 'VERB', 'ADP', 'PRON', 'ADV', 'ADJ', 'SYM', 'DET', 'ADV', 'NOUN', 'AUX', 'ADJ', 'DET', 'ADJ', 'NOUN', 'SYM', 'ADV', 'DET', 'ADJ', 'ADP', 'DET', 'NOUN', 'PRON', 'PRON', 'AUX', 'SYM', 'ADV', 'ADP', 'DET', 'NOUN', 'AUX', 'PRON', 'ADJ', 'SYM', 'CCONJ', 'SCONJ', 'PRON', 'NOUN', 'AUX', 'SYM', 'VERB', 'DET', 'NOUN', 'ADV', 'ADV', 'ADV', 'PRON', 'DET', 'NOUN', 'ADP', 'PRON', 'NOUN', 'SYM']\n"
     ]
    }
   ],
   "source": [
    "# lijsten maken\n",
    "tokens = []\n",
    "lemmas = []\n",
    "postags = []\n",
    "for token in doc:\n",
    "    tokens.append(token.text)      # voeg elk token toe aan lijst van tokens\n",
    "    lemmas.append(token.lemma_)    # voeg elk lemma toe aan lijst van lemma's\n",
    "    postags.append(token.pos_)     # voeg elke part-of-speech tag toe aan lijst 'postags'\n",
    "\n",
    "# lijsten tonen\n",
    "print(\"tokens:\")\n",
    "print(tokens)\n",
    "print(\"lemma's:\")\n",
    "print(lemmas)\n",
    "print(\"part-of-speech tags:\")\n",
    "print(postags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='color: #690027;' markdown=\"1\">\n",
    "    <h2>3. Sentiment lexicon matching</h2> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu je review *gepreprocessed* is, kan je het sentiment bepalen met behulp van het sentiment lexicon dat je ter beschikking hebt. Dit was reeds geautomatiseerd in deel 1. Je neemt de code van deel 1 over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Het sentiment van de review is: positief\n"
     ]
    }
   ],
   "source": [
    "# zoek lexicon matches in de review\n",
    "lexiconmatches = []       # lijst tokens gevonden in lexicon\n",
    "polariteiten = []         # lijst polariteiten van gevonden tokens  \n",
    "\n",
    "i = 0      # index;  index = 0 komt overeen met eerste lemma en eerste postag\n",
    "for lemma in lemmas:\n",
    "    if lemma in lexicondict.keys():  # sleutels zijn woorden aanwezig in lexicon\n",
    "        if postags[i] in lexicondict[lemma][\"postag\"]: # alleen wanneer het lemma en de POS-tag overeenkomen, is er een match (zie bv. 'fout als ADJ en 'fout' als NOUN)\n",
    "            lexiconmatches.append(tokens[i])           # overeenkomstig token toevoegen aan lijst lexiconmatches\n",
    "            polariteiten.append(sum(lexicondict[lemma][\"polarity\"]))   # overeenkomstige polariteit toevoegen aan lijst polariteiten\n",
    "    i = i + 1  # ga over naar volgende lemma, dus lemma met als index eentje meer          \n",
    "\n",
    "# toon eindbeslissing voor deze review: de som van alle polariteiten\n",
    "if sum(polariteiten) > 0:\n",
    "    sentiment = \"positief\"\n",
    "elif sum(polariteiten) == 0:\n",
    "    sentiment = \"neutraal\"\n",
    "elif sum(polariteiten) < 0:\n",
    "    sentiment = \"negatief\"\n",
    "print(\"Het sentiment van de review is: \" + sentiment)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='color: #690027;' markdown=\"1\">\n",
    "    <h2>4. Sentiment lexicon matching op eigen review</h2> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je kan dit ook doen voor een zelfgeschreven review en de output van het systeem vergelijken met je eigen annotatie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hopelijk wordt dit een leuke notebook!\n",
      "tokens:\n",
      "['hopelijk', 'wordt', 'dit', 'een', 'leuke', 'notebook', '!']\n",
      "lemma's:\n",
      "['hopelijk', 'worden', 'dit', 'een', 'leuk', 'notebook', '!']\n",
      "part-of-speech tags:\n",
      "['ADJ', 'AUX', 'PRON', 'DET', 'ADJ', 'NOUN', 'SYM']\n"
     ]
    }
   ],
   "source": [
    "# vul je review in tussen de aanhalingstekens, pas dus de gegeven string aan\n",
    "zelfgeschreven_review = \"Hopelijk wordt dit een leuke notebook!\"\n",
    "# vul de polariteit in tussen de aanhalingstekens (positief, negatief, neutraal), pas ook hier de gegeven string aan\n",
    "label = \"positief\"\n",
    "\n",
    "# volgende stappen: review tonen en nlp erop toepassen\n",
    "print(zelfgeschreven_review)\n",
    "doczg = nlp(zelfgeschreven_review.lower())\n",
    "\n",
    "# elk woord in review tonen met woordsoort en part-of-speech tag en opslaan in lijsten\n",
    "tokenszg = []\n",
    "lemmaszg = []\n",
    "postagszg = []\n",
    "for token in doczg:\n",
    "    tokenszg.append(token.text)\n",
    "    lemmaszg.append(token.lemma_)\n",
    "    postagszg.append(token.pos_)\n",
    "\n",
    "print(\"tokens:\")\n",
    "print(tokenszg)\n",
    "print(\"lemma's:\")\n",
    "print(lemmaszg)\n",
    "print(\"part-of-speech tags:\")\n",
    "print(postagszg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu de preprocessing klaar is kan je opnieuw matches zoeken met het lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2]\n",
      "1.2\n",
      "Het sentiment van de review is: positief\n"
     ]
    }
   ],
   "source": [
    "# zoek lexicon matches in de review\n",
    "lexiconmatcheszg = []       # lijst tokens gevonden in lexicon\n",
    "polariteitenzg = []         # lijst polariteiten van gevonden tokens  \n",
    "\n",
    "i = 0      # index;  index = 0 komt overeen met eerste lemma en eerste postag\n",
    "for lemma in lemmaszg:\n",
    "    if lemma in lexicondict.keys():  # sleutels zijn woorden aanwezig in lexicon\n",
    "        if postagszg[i] in lexicondict[lemma][\"postag\"]: # alleen wanneer het lemma en de POS-tag overeenkomen, is er een match (zie bv. 'fout als ADJ en 'fout' als NOUN)\n",
    "            lexiconmatcheszg.append(tokenszg[i])           # overeenkomstig token toevoegen aan lijst lexiconmatches\n",
    "            polariteitenzg.append(sum(lexicondict[lemma][\"polarity\"]))   # overeenkomstige polariteit toevoegen aan lijst polariteiten\n",
    "    i = i + 1  # ga over naar volgende lemma, dus lemma met als index eentje meer          \n",
    "\n",
    "# toon eindbeslissing voor deze review: de som van alle polariteiten\n",
    "if sum(polariteitenzg) > 0:\n",
    "    sentiment = \"positief\"\n",
    "elif sum(polariteitenzg) == 0:\n",
    "    sentiment = \"neutraal\"\n",
    "elif sum(polariteitenzg) < 0:\n",
    "    sentiment = \"negatief\"\n",
    "print(polariteitenzg)\n",
    "print(sum(polariteitenzg))\n",
    "print(\"Het sentiment van de review is: \" + sentiment)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vergelijk de eindbeslissing van het regelgebaseerde systeem met je eigen annotatie. Heeft het systeem het juist? Waarom wel/niet, denk je?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../.images/ChatbotSentiment/cclic.png\" alt=\"Banner\" align=\"left\" style=\"width:100px;\"/><br><br>\n",
    "Notebook Chatbot, zie <a href=\"http://www.aiopschool.be\">AI Op School</a>, van C. Van Hee, V. Hoste, F. wyffels, Z. Van de Staey & N. Gesquière is in licentie gegeven volgens een <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Naamsvermelding-NietCommercieel-GelijkDelen 4.0 Internationaal-licentie</a>. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
